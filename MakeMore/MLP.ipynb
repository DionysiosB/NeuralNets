{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "yG2bdIg9WrDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3          # Number of characters used to predict the next one\n",
        "B  = 26                 # Size of the alphabet\n",
        "NE = 2                  # Number of embedding dimensions\n",
        "NH = 100                # Number of nodes in the hidden layer\n",
        "BATCH_SIZE = 100        # How many samples for each batch\n",
        "NUM_ITERATIONS=10000    # How many epochs to run backprop\n",
        "learning_rate = 0.1     # Initial learning rate\n",
        "window_size = 100        # Window size to plot smoothed list of losses\n",
        "\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "0kKsn0ccRN-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getnum(x : str, default : int) -> int:\n",
        "    return (ord(x) - ord('a')) if x.isalpha() else default\n",
        "\n",
        "def build_dataset(words : List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    X, Y = list(), list()\n",
        "    for w in words:\n",
        "        if len(w) < block_size: continue\n",
        "        w = \".\" * block_size + w + '.'\n",
        "        for p in range(block_size, len(w)):\n",
        "            prev = w[(p - block_size) : p]\n",
        "            next = w[p]\n",
        "            prev = [getnum(x, B) for x in prev]\n",
        "            next = getnum(next, B)\n",
        "            X.append(prev)\n",
        "            Y.append(next)\n",
        "    return torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "Ur2JmznxSa5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\") as f: words = f.read().splitlines()\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.1 * len(words))\n",
        "n2 = int(0.9 * len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xvd, Yvd = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n",
        "\n",
        "print(Xtr.shape, Ytr.shape)\n",
        "print(Xvd.shape, Yvd.shape)\n",
        "print(Xte.shape, Yte.shape)"
      ],
      "metadata": {
        "id": "ylAwefxFSc8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(0)\n",
        "C = torch.randn((B + 1, NE), generator=g)\n",
        "Wh = torch.randn((block_size * NE, NH), generator=g)\n",
        "bh = torch.randn(NH, generator=g)\n",
        "Wt = torch.randn((NH, B + 1), generator=g)\n",
        "bt = torch.randn((B + 1), generator=g)\n",
        "\n",
        "all_params =[C, Wh, bh, Wt, bt]\n",
        "for param in all_params:\n",
        "    param.requires_grad = True\n",
        "\n",
        "num_params = sum([param.nelement() for param in all_params])\n",
        "print(f\"Total number of parameters: {num_params}\")"
      ],
      "metadata": {
        "id": "ExNLbqoEwL1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = [0] * NUM_ITERATIONS\n",
        "\n",
        "for iter in range(NUM_ITERATIONS):\n",
        "    indices = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,))\n",
        "    embeddings = C[Xtr[indices]]\n",
        "    #print(embeddings.shape)\n",
        "    hidden = torch.tanh(embeddings.view(-1, block_size * NE) @ Wh + bh)\n",
        "    logits = hidden @ Wt + bt\n",
        "    loss = F.cross_entropy(logits, Ytr[indices])\n",
        "\n",
        "    for param in all_params: param.grad = None\n",
        "    loss.backward()\n",
        "    learning_rate /= (1.0 if iter < NUM_ITERATIONS / 2 else 10)\n",
        "    for param in all_params: param.data -= learning_rate * param.grad\n",
        "\n",
        "    losses[iter] = loss.log10().item()"
      ],
      "metadata": {
        "id": "lydkLt8QXMmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smooth_losses = smoothed_data = [\n",
        "    np.mean(losses[max(0, p - window_size) : min(len(losses), p + window_size + 1)])\n",
        "    for p in range(len(losses))\n",
        "]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.plot(range(len(smooth_losses)), smooth_losses)"
      ],
      "metadata": {
        "id": "r495zgwbolba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}