{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open(\"names.txt\", \"r\").read().splitlines()\n",
        "charlist = sorted(list(set(\"\".join(words))))\n",
        "charlist = ['.'] + charlist\n",
        "numfromchar = {ch: k for k,ch in enumerate(charlist)}\n",
        "charfromnum = {k: ch for k,ch in enumerate(charlist)}\n",
        "\n",
        "#(Hyper)Parameters\n",
        "nc      = len(charlist)\n",
        "nw      = len(words)\n",
        "embdim  = 2\n",
        "blocksz = 3\n",
        "firstsz  = embdim * blocksz\n",
        "hiddensz = 100\n"
      ],
      "metadata": {
        "id": "nVglxZrWy-4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "for w in words:\n",
        "    t = ('.' * blocksz) + w + '.'\n",
        "    for k in range(blocksz, len(t)):\n",
        "        input  = t[k - blocksz : k]\n",
        "        output = t[k]\n",
        "        X.append([numfromchar[ch] for ch in input])\n",
        "        Y.append(numfromchar[output])\n",
        "        #print(input + \" -> \" + output)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "#print(X.shape, X.dtype, Y.shape, Y.dtype)"
      ],
      "metadata": {
        "id": "0QcLAbOenMnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn([nc, embdim])\n",
        "emb = C[X]"
      ],
      "metadata": {
        "id": "-ZwY0oxrnift"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.randn(firstsz, hiddensz)\n",
        "b1  = torch.randn(firstz)\n",
        "h = torch.tanh(emb.view(-1,firstsz) @ W1 + b1)"
      ],
      "metadata": {
        "id": "sTlB36xCr6_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = torch.randn(hiddensz, nc)\n",
        "b2 = torch.randn(nc)\n",
        "logits = h @ W2 + b2\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(32), Y].log().mean()"
      ],
      "metadata": {
        "id": "EFH6dN4koZ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open(\"names.txt\", \"r\").read().splitlines()\n",
        "charlist = sorted(list(set(\"\".join(words))))\n",
        "charlist = ['.'] + charlist\n",
        "numfromchar = {ch: k for k,ch in enumerate(charlist)}\n",
        "charfromnum = {k: ch for k,ch in enumerate(charlist)}\n",
        "\n",
        "#(Hyper)Parameters\n",
        "nc      = len(charlist)\n",
        "nw      = len(words)\n",
        "embdim  = 2\n",
        "blocksz = 3\n",
        "firstsz  = embdim * blocksz\n",
        "hiddensz = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "### Preprocess the raw data\n",
        "X, Y = list(), list()\n",
        "for w in words:\n",
        "    #print(\"\\n\");print(w)\n",
        "    t = ('.' * blocksz) + w + '.'\n",
        "    for k in range(blocksz, len(t)):\n",
        "        input  = t[k - blocksz : k]\n",
        "        output = t[k]\n",
        "        X.append([numfromchar[ch] for ch in input])\n",
        "        Y.append(numfromchar[output])\n",
        "        #print(input + \" -> \" + output)\n",
        "X, Y = torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "\n",
        "#Initialization\n",
        "g  = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((nc, embdim),generator=g)\n",
        "W1 = torch.randn((firstsz, hiddensz), generator = g)\n",
        "b1 = torch.randn(hiddensz, generator = g)\n",
        "W2 = torch.randn((hiddensz, nc), generator = g)\n",
        "b2 = torch.randn(nc, generator = g)\n",
        "params = [C, W1, b1, W2, b2]\n",
        "for p in params: p.requires_grad = True\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    emb = C[X]\n",
        "    h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    loss  = F.cross_entropy(logits, Y)\n",
        "    print(f\"Loss:{loss.item()}\")\n",
        "    for p in params: p.grad = None\n",
        "    loss.backward()\n",
        "    for p in params:\n",
        "        p.data -= learning_rate * p.grad"
      ],
      "metadata": {
        "id": "rCa6n4je8ulr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open(\"names.txt\", \"r\").read().splitlines()\n",
        "charlist = sorted(list(set(\"\".join(words))))\n",
        "charlist = ['.'] + charlist\n",
        "numfromchar = {ch: k for k,ch in enumerate(charlist)}\n",
        "charfromnum = {k: ch for k,ch in enumerate(charlist)}\n",
        "\n",
        "#(Hyper)Parameters\n",
        "nc      = len(charlist)\n",
        "nw      = len(words)\n",
        "embdim  = 2\n",
        "blocksz = 3\n",
        "firstsz  = embdim * blocksz\n",
        "hiddensz = 100\n",
        "batchsz = 32\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "### Preprocess the raw data\n",
        "X, Y = list(), list()\n",
        "for w in words:\n",
        "    #print(\"\\n\");print(w)\n",
        "    t = ('.' * blocksz) + w + '.'\n",
        "    for k in range(blocksz, len(t)):\n",
        "        input  = t[k - blocksz : k]\n",
        "        output = t[k]\n",
        "        X.append([numfromchar[ch] for ch in input])\n",
        "        Y.append(numfromchar[output])\n",
        "        #print(input + \" -> \" + output)\n",
        "X, Y = torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "\n",
        "#Initialization\n",
        "g  = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((nc, embdim),generator=g)\n",
        "W1 = torch.randn((firstsz, hiddensz), generator = g)\n",
        "b1 = torch.randn(hiddensz, generator = g)\n",
        "W2 = torch.randn((hiddensz, nc), generator = g)\n",
        "b2 = torch.randn(nc, generator = g)\n",
        "params = [C, W1, b1, W2, b2]\n",
        "for p in params: p.requires_grad = True\n",
        "\n",
        "\n",
        "for _ in range(epochs):\n",
        "    batch_idx = torch.randint(0, X.shape[0], (batchsz,))\n",
        "    emb = C[X[batch_idx]]\n",
        "    h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    loss  = F.cross_entropy(logits, Y[batch_idx])\n",
        "    print(f\"Loss:{loss.item()}\")\n",
        "    for p in params: p.grad = None\n",
        "    loss.backward()\n",
        "    for p in params:\n",
        "        p.data -= learning_rate * p.grad"
      ],
      "metadata": {
        "id": "qhsn4zJo_jn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Learning rate\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open(\"names.txt\", \"r\").read().splitlines()\n",
        "charlist = sorted(list(set(\"\".join(words))))\n",
        "charlist = ['.'] + charlist\n",
        "numfromchar = {ch: k for k,ch in enumerate(charlist)}\n",
        "charfromnum = {k: ch for k,ch in enumerate(charlist)}\n",
        "\n",
        "#(Hyper)Parameters\n",
        "nc      = len(charlist)\n",
        "nw      = len(words)\n",
        "embdim  = 2\n",
        "blocksz = 3\n",
        "firstsz  = embdim * blocksz\n",
        "hiddensz = 100\n",
        "batchsz = 32\n",
        "learning_rates = torch.logspace(-3,0,50)\n",
        "epochs = 2000\n",
        "\n",
        "\n",
        "### Preprocess the raw data\n",
        "X, Y = list(), list()\n",
        "for w in words:\n",
        "    #print(\"\\n\");print(w)\n",
        "    t = ('.' * blocksz) + w + '.'\n",
        "    for k in range(blocksz, len(t)):\n",
        "        input  = t[k - blocksz : k]\n",
        "        output = t[k]\n",
        "        X.append([numfromchar[ch] for ch in input])\n",
        "        Y.append(numfromchar[output])\n",
        "        #print(input + \" -> \" + output)\n",
        "X, Y = torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "\n",
        "#Initialization\n",
        "g  = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((nc, embdim),generator=g)\n",
        "W1 = torch.randn((firstsz, hiddensz), generator = g)\n",
        "b1 = torch.randn(hiddensz, generator = g)\n",
        "W2 = torch.randn((hiddensz, nc), generator = g)\n",
        "b2 = torch.randn(nc, generator = g)\n",
        "params = [C, W1, b1, W2, b2]\n",
        "for p in params: p.requires_grad = True\n",
        "\n",
        "\n",
        "res = [1000.0] * len(learning_rates)\n",
        "for lridx in range(len(learning_rates)):\n",
        "    learning_rate = learning_rates[lridx]\n",
        "    for _ in range(epochs):\n",
        "        batch_idx = torch.randint(0, X.shape[0], (batchsz,))\n",
        "        emb = C[X[batch_idx]]\n",
        "        h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "        logits = h @ W2 + b2\n",
        "        loss  = F.cross_entropy(logits, Y[batch_idx])\n",
        "        #print(f\"Loss:{loss.item()}\")\n",
        "        for p in params: p.grad = None\n",
        "        loss.backward()\n",
        "        for p in params:\n",
        "            p.data -= learning_rate * p.grad\n",
        "\n",
        "    print(f\"Learningrate:{learning_rate} -> Loss:{loss.item()}\")\n",
        "    res[lridx] = loss.item()\n",
        "\n",
        "plt.plot(learning_rates, res)"
      ],
      "metadata": {
        "id": "BH0d8mV5_nsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Train/Validation/Test Split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "words = open(\"names.txt\", \"r\").read().splitlines()\n",
        "charlist = sorted(list(set(\"\".join(words))))\n",
        "charlist = ['.'] + charlist\n",
        "numfromchar = {ch: k for k,ch in enumerate(charlist)}\n",
        "charfromnum = {k: ch for k,ch in enumerate(charlist)}\n",
        "\n",
        "#(Hyper)Parameters\n",
        "nc      = len(charlist)\n",
        "nw      = len(words)\n",
        "embdim  = 2\n",
        "blocksz = 3\n",
        "firstsz  = embdim * blocksz\n",
        "hiddensz = 100\n",
        "batchsz = 32\n",
        "learning_rate = 0.1\n",
        "epochs = 50000\n",
        "split_fractions = [0.8, 0.9, 1.0]\n",
        "\n",
        "### Preprocess the raw data\n",
        "X, Y = list(), list()\n",
        "for w in words:\n",
        "    #print(\"\\n\");print(w)\n",
        "    t = ('.' * blocksz) + w + '.'\n",
        "    for k in range(blocksz, len(t)):\n",
        "        input  = t[k - blocksz : k]\n",
        "        output = t[k]\n",
        "        X.append([numfromchar[ch] for ch in input])\n",
        "        Y.append(numfromchar[output])\n",
        "        #print(input + \" -> \" + output)\n",
        "X, Y = torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "### Train/Validation/Test split of X and Y\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "idxs = np.random.permutation(len(words))\n",
        "ntr = int(split_fractions[0] * len(words))\n",
        "nvd = int(split_fractions[1] * len(words))\n",
        "tridxs  = idxs[:ntr]\n",
        "vdidxs  = idxs[ntr:nvd]\n",
        "tstidxs = idxs[nvd:]\n",
        "\n",
        "Xtr, Ytr = X[tridxs], Y[tridxs]\n",
        "Xvd, Yvd = X[vdidxs], Y[vdidxs]\n",
        "Xtst, Ytst = X[tstidxs], Y[tstidxs]\n",
        "\n",
        "\n",
        "#Initialization\n",
        "g  = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((nc, embdim),generator=g)\n",
        "W1 = torch.randn((firstsz, hiddensz), generator = g)\n",
        "b1 = torch.randn(hiddensz, generator = g)\n",
        "W2 = torch.randn((hiddensz, nc), generator = g)\n",
        "b2 = torch.randn(nc, generator = g)\n",
        "params = [C, W1, b1, W2, b2]\n",
        "for p in params: p.requires_grad = True\n",
        "\n",
        "###Training\n",
        "for _ in range(epochs):\n",
        "    batch_idx = torch.randint(0, Xtr.shape[0], (batchsz,))\n",
        "    emb = C[Xtr[batch_idx]]\n",
        "    h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "    logits = h @ W2 + b2\n",
        "    loss  = F.cross_entropy(logits, Ytr[batch_idx])\n",
        "    #print(f\"Loss:{loss.item()}\")\n",
        "    for p in params: p.grad = None\n",
        "    loss.backward()\n",
        "    for p in params:\n",
        "        p.data -= torch.tensor(learning_rate) * p.grad\n",
        "    #print(f\"Training Loss:{loss.item()}\")\n",
        "\n",
        "\n",
        "###Evaluation\n",
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss  = F.cross_entropy(logits, Ytr)\n",
        "print(f\"Training Loss:{loss.item()}\")\n",
        "emb = C[Xvd]\n",
        "h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss  = F.cross_entropy(logits, Yvd)\n",
        "print(f\"Validation Loss:{loss.item()}\")\n"
      ],
      "metadata": {
        "id": "LyfoK-L0V0w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Visualize Embeddings\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.scatter(C[:,0].data, C[:,1].data,s=200)\n",
        "for k in range(C.shape[0]):\n",
        "    plt.text(C[k, 0].item(), C[k, 1].item(), charfromnum[k], ha=\"center\", va=\"center\",color=\"white\")\n",
        "plt.grid('minor')"
      ],
      "metadata": {
        "id": "VB7bQyfgRQXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Use the network to actually make more :-)\n",
        "\n",
        "numout = 10\n",
        "for _ in range(numout):\n",
        "    input  = [0] * blocksz\n",
        "    output = []\n",
        "    while True:\n",
        "        emb = C[input]\n",
        "        h = torch.tanh(emb.view(-1, firstsz) @ W1 + b1)\n",
        "        logits = h @ W2 + b2\n",
        "        probs = F.softmax(logits,dim = 1)\n",
        "        pred = torch.multinomial(probs, num_samples=1,generator=g).item()\n",
        "        input = input[1:] + [pred]\n",
        "        if not pred: break\n",
        "        output.append(pred)\n",
        "\n",
        "    res = \"\".join([charfromnum[k] for k in output])\n",
        "    print(res)\n"
      ],
      "metadata": {
        "id": "mYQHvayzSTqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CoB5xlFi_thy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}